import streamlit as st
import google.generativeai as genai
from openai import OpenAI
from datetime import datetime
import json
import os
import concurrent.futures
import PyPDF2  # PDF ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from io import StringIO

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Dual-AI Hub (Pro)", layout="wide")
st.title("âš¡ Dual-AI Insight Hub")

# --- 2. API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
except KeyError:
    st.error("ğŸš¨ API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=gemini_api_key)
gpt_client = OpenAI(api_key=gpt_api_key)

# --- 3. ëª¨ë¸ ì„¤ì • ---
def get_best_available_model():
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        priority_keywords = ['1.5-flash', '1.5-pro', 'gemini-pro']
        for keyword in priority_keywords:
            for m in models:
                if keyword in m: return m
        return models[0] if models else "models/gemini-pro"
    except: return "models/gemini-pro"

GEMINI_MODEL = get_best_available_model()
GPT_MODEL = "gpt-4o-mini"

# --- 4. ë°ì´í„° ê´€ë¦¬ ---
DB_FILE = "chat_db.json"

def load_data():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
    return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]

def save_data(sessions):
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(sessions, f, ensure_ascii=False, indent=4)

# --- 5. [í•µì‹¬] íŒŒì¼ ì½ê¸° í•¨ìˆ˜ ---
def read_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # 1. PDF íŒŒì¼
        if uploaded_file.type == "application/pdf":
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        
        # 2. í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ (txt, csv, py, md ë“±)
        else:
            stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
            return stringio.read()
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"

# --- 6. ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ---
if "sessions" not in st.session_state:
    st.session_state.sessions = load_data()
    st.session_state.active_index = 0

if "system_role" not in st.session_state:
    st.session_state.system_role = "ë„ˆëŠ” ê° ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ë‹¤. ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ë¼."

if "active_index" not in st.session_state:
    st.session_state.active_index = 0

def get_active_session():
    if st.session_state.active_index >= len(st.session_state.sessions):
        st.session_state.active_index = 0
    return st.session_state.sessions[st.session_state.active_index]

# --- 7. ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ ---
def call_gemini(prompt):
    model = genai.GenerativeModel(GEMINI_MODEL)
    return model.generate_content(prompt).text

def call_gpt(messages):
    response = gpt_client.chat.completions.create(model=GPT_MODEL, messages=messages)
    return response.choices[0].message.content

# --- 8. ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ® ì œì–´ ì„¼í„°")
    
    with st.expander("ğŸ­ AI ì—­í• (Persona) ì„¤ì •"):
        input_role = st.text_area("ì—­í•  ì…ë ¥", value=st.session_state.system_role, height=70)
        if st.button("ğŸ’¾ ì—­í•  ì ìš©", use_container_width=True):
            st.session_state.system_role = input_role
            st.success("ì ìš© ì™„ë£Œ!")

    st.divider()

    # [NEW] íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
    st.subheader("ğŸ“‚ ìë£Œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ë¶„ì„í•  íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=["pdf", "txt", "csv", "py", "md"])
    file_content = ""
    
    if uploaded_file is not None:
        with st.spinner("íŒŒì¼ ì½ëŠ” ì¤‘..."):
            file_content = read_uploaded_file(uploaded_file)
        st.success(f"âœ… {uploaded_file.name} ë¡œë“œ ì™„ë£Œ! ({len(file_content)}ì)")
        with st.expander("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
            st.text(file_content[:500] + "...") # ì•ë¶€ë¶„ë§Œ ì‚´ì§ ë³´ì—¬ì¤Œ

    st.divider()

    # ëŒ€í™”ë°© ê´€ë¦¬
    st.subheader("ğŸ—‚ï¸ ëŒ€í™”ë°© ê´€ë¦¬")
    active_session = get_active_session()
    
    new_title = st.text_input("ğŸ·ï¸ ë°© ì´ë¦„ ìˆ˜ì •", value=active_session["title"], key=f"title_{st.session_state.active_index}")
    if new_title != active_session["title"]:
        active_session["title"] = new_title
        save_data(st.session_state.sessions)
        st.rerun()

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• ìƒˆ ëŒ€í™”", use_container_width=True):
            new_session = {"title": "ìƒˆ ëŒ€í™”", "history": []}
            st.session_state.sessions.insert(0, new_session)
            st.session_state.active_index = 0
            save_data(st.session_state.sessions)
            st.rerun()
    with col2:
        if st.button("ğŸ—‘ï¸ ì „ì²´ ì‚­ì œ", use_container_width=True):
            if os.path.exists(DB_FILE): os.remove(DB_FILE)
            st.session_state.sessions = [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
            st.session_state.active_index = 0
            st.rerun()

    st.markdown("---")
    for i, session in enumerate(st.session_state.sessions):
        label = session["title"]
        if len(label) > 15: label = label[:15] + "..."
        if i == st.session_state.active_index:
            st.button(f"ğŸ“‚ {label}", key=f"s_{i}", use_container_width=True, disabled=True)
        else:
            if st.button(f"ğŸ“„ {label}", key=f"s_{i}", use_container_width=True):
                st.session_state.active_index = i
                st.rerun()

# --- 9. ë©”ì¸ ë¡œì§ ---
active_session = get_active_session()
chat_history = active_session["history"]
current_role = st.session_state.system_role

# ì•ˆë‚´ ë¬¸êµ¬ (íŒŒì¼ì´ ì˜¬ë¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ)
if uploaded_file:
    st.info(f"ğŸ“ **{uploaded_file.name}** íŒŒì¼ì„ ì°¸ì¡°í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    if len(chat_history) == 0 and active_session["title"] == "ìƒˆ ëŒ€í™”":
        active_session["title"] = user_input
        save_data(st.session_state.sessions)
        st.rerun()

    with st.status("âš¡ íŒŒì¼ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...", expanded=True) as status:
        turn_data = {"q": user_input, "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M")}
        
        # [í•µì‹¬] íŒŒì¼ ë‚´ìš©ì´ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í•©ì¹˜ê¸°
        final_user_content = user_input
        if file_content:
            final_user_content = f"""
            [ì°¸ê³  ìë£Œ ì‹œì‘]
            {file_content}
            [ì°¸ê³  ìë£Œ ë]
            
            [ì‚¬ìš©ì ì§ˆë¬¸]: {user_input}
            """

        with concurrent.futures.ThreadPoolExecutor() as executor:
            # STEP 1: ë™ì‹œ ë‹µë³€
            st.write(f"1ï¸âƒ£ ìë£Œ ë¶„ì„ ë° ë‹µë³€ ìƒì„± (Role: {current_role[:10]}...)")
            
            # Gemini Prompt
            g_msg = f"System Instruction: {current_role}\n\n{final_user_content}"
            # GPT Messages
            o_msg = [{"role": "system", "content": current_role}, {"role": "user", "content": final_user_content}]
            
            f_g = executor.submit(call_gemini, g_msg)
            f_o = executor.submit(call_gpt, o_msg)
            
            turn_data["g_resp"] = f_g.result()
            turn_data["o_resp"] = f_o.result()

            # STEP 2: ë™ì‹œ ë¶„ì„
            st.write("2ï¸âƒ£ ìƒí˜¸ ë¹„í‰ ì¤‘...")
            
            # ë¹„í‰í•  ë•ŒëŠ” íŒŒì¼ ë‚´ìš©ì„ êµ³ì´ ë‹¤ì‹œ ì¤„ í•„ìš” ì—†ìŒ (ë‹µë³€ë§Œ ë³´ë©´ ë˜ë‹ˆê¹Œ)
            # í•˜ì§€ë§Œ ì •í™•ì„±ì„ ìœ„í•´ ì›ë³¸ ì§ˆë¬¸ ë§¥ë½ì€ ìœ ì§€
            
            g_an_prompt = f"[Role]: {current_role}\n[Task]: Critically evaluate GPT's answer based on the user's question and context.\n\n[User Question]: {user_input}\n[GPT Answer]: {turn_data['o_resp']}"
            
            o_an_msg = [{"role": "system", "content": current_role}, {"role": "user", "content": f"Evaluate Gemini's answer based on the user's question.\n\n[User Question]: {user_input}\n[Gemini Answer]: {turn_data['g_resp']}"}]
            
            f_g_an = executor.submit(call_gemini, g_an_prompt)
            f_o_an = executor.submit(call_gpt, o_an_msg)
            
            turn_data["g_an"] = f_g_an.result()
            turn_data["o_an"] = f_o_an.result()

            # STEP 3: ê²°ë¡ 
            st.write("3ï¸âƒ£ ìµœì¢… ê²°ë¡  ë„ì¶œ...")
            final_prompt = f"""
            Role: {current_role}
            Task: Synthesize a final solution based on the discussion.
            Requirement: Fix errors pointed out in the reviews.
            
            Q: {user_input}
            Gemini: {turn_data['g_resp']}
            GPT: {turn_data['o_resp']}
            Review(G): {turn_data['g_an']}
            Review(O): {turn_data['o_an']}
            """
            turn_data["final_con"] = call_gpt([{"role": "user", "content": final_prompt}])

            active_session["history"].append(turn_data)
            save_data(st.session_state.sessions)
            status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
            st.rerun()

# --- 10. ê²°ê³¼ ì¶œë ¥ ---
if chat_history:
    st.caption(f"ğŸ•’ {len(chat_history)}ê°œì˜ ê¸°ë¡ | í˜„ì¬ ëŒ€í™”ë°©: {active_session['title']}")
    
    for i, chat in enumerate(reversed(chat_history)):
        idx = len(chat_history) - i
        st.markdown(f"### Q{idx}. {chat['q']}")
        
        t1, t2, t3 = st.tabs(["ğŸ’¬ ë‹µë³€", "âš”ï¸ ë¹„í‰", "ğŸ† ê²°ë¡ "])
        with t1:
            c1, c2 = st.columns(2)
            with c1: st.info("ğŸ’ ë‹¤ì˜¨"); st.write(chat['g_resp'])
            with c2: st.success("ğŸ§  ë£¨"); st.write(chat['o_resp'])
        with t2:
            c1, c2 = st.columns(2)
            with c1: st.info("ë¹„í‰"); st.write(chat['g_an'])
            with c2: st.success("í‰ê°€"); st.write(chat['o_an'])
        with t3: st.markdown(chat['final_con'])
        st.divider()
