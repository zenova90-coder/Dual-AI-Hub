import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import json
import os
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Dual-AI Hub", layout="wide")
st.title("ğŸ¤– Dual-AI Insight Hub")

# --- 1. íŒŒì¼ ê¸°ë°˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ ---
HISTORY_FILE = "chat_history.json"

def load_history():
    if not os.path.exists(HISTORY_FILE):
        return []
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def save_history(new_entry):
    history = load_history()
    history.insert(0, new_entry) # ìµœì‹  ê¸€ì„ ë§¨ ìœ„ë¡œ
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def delete_history():
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)

# --- 2. API í‚¤ ë° ëª¨ë¸ ì„¤ì • ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
except FileNotFoundError:
    st.error("ğŸš¨ Secrets ì„¤ì •ì´ ì•ˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=gemini_api_key)
gpt_client = OpenAI(api_key=gpt_api_key)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "current_view" not in st.session_state: 
    # í˜„ì¬ í™”ë©´ì— ë³´ì—¬ì¤„ ë°ì´í„° (ì§ˆë¬¸, ë‹µë³€, ë¶„ì„, ê²°ë¡ )
    st.session_state.current_view = {
        "q": "", "g_resp": "", "o_resp": "", 
        "g_an": "", "o_an": "", "final_con": "",
        "model_name": ""
    }

# --- 3. [ë‹¥í„° ë‹¤ì˜¨] ëª¨ë¸ ì„ íƒ ë¡œì§ ---
def get_available_gemini_model():
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        preferred_order = ['models/gemini-1.5-flash', 'models/gemini-pro', 'models/gemini-1.0-pro']
        for model in preferred_order:
            if model in available_models:
                return model
        if available_models: return available_models[0]
        return None
    except: return None

valid_model_name = get_available_gemini_model()
if not valid_model_name: valid_model_name = "gemini-pro"

# --- 4. ì‚¬ì´ë“œë°” (ê¸°ë¡ ë³´ê´€ì†Œ) ---
with st.sidebar:
    st.header("ğŸ—‚ï¸ ëŒ€í™” ê¸°ë¡ (History)")
    
    if st.button("â• ìƒˆ ëŒ€í™” ì‹œì‘í•˜ê¸°", use_container_width=True):
        st.session_state.current_view = {k: "" for k in st.session_state.current_view}
        st.rerun()

    st.divider()

    # ì €ì¥ëœ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    history_data = load_history()
    
    if not history_data:
        st.caption("ì•„ì§ ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for idx, item in enumerate(history_data):
            # ë²„íŠ¼ ì´ë¦„ì€ ì§ˆë¬¸ì˜ ì• 15ê¸€ì + ì‹œê°„
            btn_label = f"{item['timestamp']} | {item['q'][:10]}..."
            if st.button(btn_label, key=f"hist_{idx}", use_container_width=True):
                # ì„ íƒí•œ ê¸°ë¡ì„ ë©”ì¸ í™”ë©´ì— ë¡œë“œ
                st.session_state.current_view = item

    st.divider()
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì‚­ì œ"):
        delete_history()
        st.rerun()

# --- 5. ë©”ì¸ ë¡œì§ (ìë™í™” í”„ë¡œì„¸ìŠ¤) ---

# ì±„íŒ… ì…ë ¥ì°½
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ [ë‹µë³€ -> ë¶„ì„ -> ê²°ë¡ ]ì´ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")

if user_input:
    # 1. ìƒíƒœì°½ ì—´ê¸° (ì§„í–‰ìƒí™© ì¤‘ê³„)
    with st.status("ğŸš€ AI í”„ë¡œì„¸ìŠ¤ ê°€ë™ ì¤‘...", expanded=True) as status:
        
        # ë°ì´í„° ì„ì‹œ ì €ì¥ì†Œ
        current_data = {
            "q": user_input,
            "timestamp": datetime.now().strftime("%m/%d %H:%M"),
            "model_name": valid_model_name
        }

        # --- STEP 1: ë‹µë³€ ìƒì„± ---
        st.write("1ï¸âƒ£ 1ë‹¨ê³„: ë‹¤ì˜¨ê³¼ ë£¨ê°€ ë‹µë³€ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ë‹¤ì˜¨ (Gemini)
        try:
            model = genai.GenerativeModel(valid_model_name.replace('models/', '')) 
            g_res = model.generate_content(user_input)
            current_data["g_resp"] = g_res.text
        except Exception as e:
            current_data["g_resp"] = f"ì—ëŸ¬: {e}"

        # ë£¨ (GPT)
        try:
            o_res = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ëƒ‰ì² í•˜ê³  ë…¼ë¦¬ì ì¸ ì „ë¬¸ê°€ 'ë£¨'ë‹¤."},
                    {"role": "user", "content": user_input}
                ]
            )
            current_data["o_resp"] = o_res.choices[0].message.content
        except Exception as e:
            current_data["o_resp"] = f"ì—ëŸ¬: {e}"
            
        # --- STEP 2: êµì°¨ ë¶„ì„ ---
        st.write("2ï¸âƒ£ 2ë‹¨ê³„: ì„œë¡œì˜ ë‹µë³€ì„ ë¹„íŒì ìœ¼ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        
        # ë‹¤ì˜¨ -> ë£¨ ë¶„ì„
        try:
            prompt = f"ë‹¤ìŒì€ 'ë£¨(GPT)'ì˜ ë‹µë³€ì´ë‹¤. ë…¼ë¦¬ì  í—ˆì ì´ë‚˜ ë³´ì™„í•  ì ì„ ë¹„íŒí•´ì¤˜:\n{current_data['o_resp']}"
            g_an = model.generate_content(prompt)
            current_data["g_an"] = g_an.text
        except: current_data["g_an"] = "ë¶„ì„ ì‹¤íŒ¨"

        # ë£¨ -> ë‹¤ì˜¨ ë¶„ì„
        try:
            prompt = f"ë‹¤ìŒì€ 'ë‹¤ì˜¨(Gemini)'ì˜ ë‹µë³€ì´ë‹¤. ì°½ì˜ì„±ê³¼ ê°ì„±, ë…¼ë¦¬ì„±ì„ í‰ê°€í•´ì¤˜:\n{current_data['g_resp']}"
            o_an = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role":"user","content":prompt}]
            )
            current_data["o_an"] = o_an.choices[0].message.content
        except: current_data["o_an"] = "ë¶„ì„ ì‹¤íŒ¨"

        # --- STEP 3: ìµœì¢… ê²°ë¡  ---
        st.write("3ï¸âƒ£ 3ë‹¨ê³„: ë£¨(GPT)ê°€ ì˜ì‚¬ë´‰ì„ ì¡ê³  ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤...")
        
        try:
            final_prompt = f"""
            ë„ˆëŠ” ìµœì¢… ì˜ì‚¬ê²°ì •ê¶Œìë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ëª…ì¾Œí•œ ê²°ë¡ ì„ ë‚´ë ¤ë¼.
            
            [ì§ˆë¬¸] {current_data['q']}
            [ë‹¤ì˜¨ ë‹µë³€] {current_data['g_resp']}
            [ë£¨ ë‹µë³€] {current_data['o_resp']}
            [ë‹¤ì˜¨ ë¹„í‰] {current_data['g_an']}
            [ë£¨ ë¹„í‰] {current_data['o_an']}
            
            ì‘ì„± ê°€ì´ë“œ:
            1. í•µì‹¬ ìŸì  ìš”ì•½
            2. ì–‘ì¸¡ ì˜ê²¬ì˜ ì¥ë‹¨ì  ë¹„êµ
            3. ìµœì¢… ì¡°ì–¸ (êµ¬ì²´ì ìœ¼ë¡œ)
            """
            final_res = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": final_prompt}]
            )
            current_data["final_con"] = final_res.choices[0].message.content
        except: current_data["final_con"] = "ê²°ë¡  ë„ì¶œ ì‹¤íŒ¨"

        # --- ì €ì¥ ë° ì¢…ë£Œ ---
        save_history(current_data) # íŒŒì¼ì— ì €ì¥
        st.session_state.current_view = current_data # í™”ë©´ì— í‘œì‹œ
        
        status.update(label="âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", state="complete", expanded=False)

# --- 6. í™”ë©´ ì¶œë ¥ (íƒ­ êµ¬ì„±) ---

# ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í™”ë©´ í‘œì‹œ
if st.session_state.current_view["q"]:
    st.subheader(f"ğŸ—£ï¸ ì§ˆë¬¸: {st.session_state.current_view['q']}")
    
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ 1. ì˜ê²¬ ì œì‹œ", "âš”ï¸ 2. êµì°¨ ê²€ì¦", "ğŸ† 3. ìµœì¢… ê²°ë¡ "])

    with tab1:
        c1, c2 = st.columns(2)
        with c1:
            st.info(f"ğŸ’ ë‹¤ì˜¨ ({st.session_state.current_view['model_name']})")
            st.write(st.session_state.current_view["g_resp"])
        with c2:
            st.success("ğŸ§  ë£¨ (GPT-4o)")
            st.write(st.session_state.current_view["o_resp"])

    with tab2:
        c1, c2 = st.columns(2)
        with c1:
            st.info("ğŸ’ ë‹¤ì˜¨ì˜ ë¹„í‰")
            st.write(st.session_state.current_view["g_an"])
        with c2:
            st.success("ğŸ§  ë£¨ì˜ í‰ê°€")
            st.write(st.session_state.current_view["o_an"])

    with tab3:
        st.markdown("### ğŸ“ ì¢…í•© ê²°ë¡  ë³´ê³ ì„œ")
        st.markdown(st.session_state.current_view["final_con"])
else:
    # ì´ˆê¸° í™”ë©´ ì•ˆë‚´
    st.info("ğŸ‘‹ ì‚¬ìš©ìë‹˜ ë°˜ê°‘ìŠµë‹ˆë‹¤. í•˜ë‹¨ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ìë™ ë¶„ì„ & ê¸°ë¡ ì €ì¥)")
