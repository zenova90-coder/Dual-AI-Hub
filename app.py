import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import json
import os
from datetime import datetime
import time

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Dual-AI Hub (Pro)", layout="wide")
st.title("ğŸ¤– Dual-AI Insight Hub (Pro Edition)")

# --- 2. API í‚¤ ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
except KeyError:
    st.error("ğŸš¨ Secrets ì„¤ì •ì´ ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

genai.configure(api_key=gemini_api_key)
gpt_client = OpenAI(api_key=gpt_api_key)

# --- 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ëˆ„ì  ì €ì¥ì†Œ) ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# --- 4. [Pro ì „ìš©] ê³ ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ ë¡œì§ ---
def get_best_gemini_model():
    """
    Pro ëª¨ë“œì´ë¯€ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ 1.5 Pro ëª¨ë¸ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    """
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        # PRO ì‚¬ìš©ìë¥¼ ìœ„í•œ ìš°ì„ ìˆœìœ„: 1.5 Pro (ì„±ëŠ¥) -> 1.5 Flash (ì†ë„) -> 1.0 Pro (êµ¬ë²„ì „)
        priority_list = [
            'models/gemini-1.5-pro',        # ìµœì‹  ê³ ì„±ëŠ¥
            'models/gemini-1.5-pro-latest', # ìµœì‹  ê³ ì„±ëŠ¥
            'models/gemini-1.5-flash',      # ë¹ ë¦„
            'models/gemini-pro'             # êµ¬ë²„ì „
        ]
        
        for p_model in priority_list:
            if p_model in models:
                return p_model
        
        return models[0] if models else "models/gemini-pro"
    except:
        return "models/gemini-pro"

current_model = get_best_gemini_model()

# --- 5. ì‚¬ì´ë“œë°” (ì»¨íŠ¸ë¡¤ íŒ¨ë„) ---
with st.sidebar:
    st.header("ğŸ—‚ï¸ ì œì–´ ì„¼í„°")
    
    if st.button("â• ìƒˆ ëŒ€í™” ì‹œì‘í•˜ê¸° (í™”ë©´ ì´ˆê¸°í™”)", use_container_width=True):
        st.session_state.chat_history = []
        st.rerun()
    
    st.divider()
    st.success(f"ğŸ’ í˜„ì¬ ì—°ê²°ëœ ë‹¤ì˜¨: {current_model}")
    st.caption("Pro ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ë” ê¹Šì´ ìˆëŠ” ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# --- 6. ë©”ì¸ ë¡œì§: ì§ˆë¬¸ ì…ë ¥ ë° 3ë‹¨ê³„ ìë™í™” ---
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. [ë‹µë³€ -> êµì°¨ë¶„ì„ -> ê²°ë¡ ]ì´ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")

if user_input:
    # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ëŠ” ìƒíƒœì°½
    with st.status("ğŸš€ ê³ ì„±ëŠ¥ AI í”„ë¡œì„¸ìŠ¤ ê°€ë™ ì¤‘...", expanded=True) as status:
        
        # ì´ë²ˆ í„´ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        turn_data = {
            "q": user_input,
            "timestamp": datetime.now().strftime("%H:%M:%S"),
            "model": current_model
        }

        try:
            # -------------------------------------------------------
            # [STEP 1] ë‹µë³€ ìƒì„± (Generation)
            # -------------------------------------------------------
            st.write("1ï¸âƒ£ ë‹¤ì˜¨(Gemini Pro)ê³¼ ë£¨(GPT-4o)ê°€ ë‹µë³€ì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤...")
            
            # ë‹¤ì˜¨ (Gemini)
            gemini = genai.GenerativeModel(current_model.replace('models/', ''))
            g_res = gemini.generate_content(user_input)
            turn_data["g_resp"] = g_res.text
            
            # ë£¨ (GPT)
            o_res = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ëƒ‰ì² í•˜ê³  ë…¼ë¦¬ì ì¸ ì „ë¬¸ê°€ 'ë£¨'ë‹¤."},
                    {"role": "user", "content": user_input}
                ]
            )
            turn_data["o_resp"] = o_res.choices[0].message.content

            # -------------------------------------------------------
            # [STEP 2] êµì°¨ ë¶„ì„ (Critique)
            # -------------------------------------------------------
            st.write("2ï¸âƒ£ ì„œë¡œì˜ ë‹µë³€ì„ ë‚ ì¹´ë¡­ê²Œ ë¹„í‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

            # ë‹¤ì˜¨ì´ ë£¨ë¥¼ ë¹„í‰
            g_critique_prompt = f"ë‹¤ìŒì€ 'ë£¨(GPT)'ì˜ ë‹µë³€ì´ë‹¤. ë…¼ë¦¬ì  í—›ì , í¸í–¥ì„±, í˜¹ì€ ë³´ì™„í•  ì ì„ ë‚ ì¹´ë¡­ê²Œ ì§€ì í•´ë¼:\n\n{turn_data['o_resp']}"
            g_an = gemini.generate_content(g_critique_prompt)
            turn_data["g_an"] = g_an.text

            # ë£¨ê°€ ë‹¤ì˜¨ì„ ë¹„í‰
            o_critique_prompt = f"ë‹¤ìŒì€ 'ë‹¤ì˜¨(Gemini)'ì˜ ë‹µë³€ì´ë‹¤. ì°½ì˜ì„±, ê°ì„±, ê·¸ë¦¬ê³  ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ í‰ê°€í•˜ê³  ë¶€ì¡±í•œ ì ì„ ì§€ì í•´ë¼:\n\n{turn_data['g_resp']}"
            o_an = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": o_critique_prompt}]
            )
            turn_data["o_an"] = o_an.choices[0].message.content

            # -------------------------------------------------------
            # [STEP 3] ìµœì¢… ê²°ë¡  (Conclusion)
            # -------------------------------------------------------
            st.write("3ï¸âƒ£ ë£¨(GPT)ê°€ ì˜ì‚¬ë´‰ì„ ì¡ê³  ìµœì¢… íŒê²°ì„ ë‚´ë¦½ë‹ˆë‹¤...")

            final_prompt = f"""
            ë„ˆëŠ” ì´ í† ë¡ ì˜ ìµœì¢… ì˜ì‚¬ê²°ì •ê¶Œì(íŒì‚¬)ë‹¤.
            ì•„ë˜ì˜ [ì§ˆë¬¸], [ë‘ AIì˜ ë‹µë³€], [ìƒí˜¸ ë¹„íŒ] ë‚´ìš©ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬
            ì‚¬ìš©ìê°€ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì™„ë²½í•œ 'ìµœì¢… ê²°ë¡ 'ì„ ë‚´ë ¤ë¼.

            [ì§ˆë¬¸] {turn_data['q']}
            [ë‹¤ì˜¨ ë‹µë³€] {turn_data['g_resp']}
            [ë£¨ ë‹µë³€] {turn_data['o_resp']}
            [ë‹¤ì˜¨ ë¹„í‰] {turn_data['g_an']}
            [ë£¨ ë¹„í‰] {turn_data['o_an']}
            
            ê²°ë¡ ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•œë‹¤.
            """
            final_res = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": final_prompt}]
            )
            turn_data["final_con"] = final_res.choices[0].message.content

            # -------------------------------------------------------
            # [ì €ì¥] ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ê¸°ë¡ì— ì¶”ê°€
            # -------------------------------------------------------
            st.session_state.chat_history.append(turn_data)
            status.update(label="âœ… ë¶„ì„ ì™„ë£Œ! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.", state="complete", expanded=False)

        except Exception as e:
            st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            status.update(label="âš ï¸ ì˜¤ë¥˜ ë°œìƒ", state="error")

# --- 7. í™”ë©´ ì¶œë ¥ (íƒ­ ëˆ„ì  ë°©ì‹) ---
if st.session_state.chat_history:
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ 1. ì˜ê²¬ ëŒ€ë¦½", "âš”ï¸ 2. êµì°¨ ê²€ì¦", "ğŸ† 3. ìµœì¢… ê²°ë¡ "])

    # ëˆ„ì ëœ ëŒ€í™” ê¸°ë¡ì„ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    for i, chat in enumerate(st.session_state.chat_history):
        # ê° ì§ˆë¬¸ë§ˆë‹¤ êµ¬ë¶„ì„ ìœ„í•œ ë²ˆí˜¸ì™€ ì§ˆë¬¸ í‘œì‹œ
        idx = i + 1
        
        with tab1:
            st.markdown(f"### Q{idx}. {chat['q']}")
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"ğŸ’ ë‹¤ì˜¨ ({chat['model']})")
                st.markdown(chat['g_resp'])
            with col2:
                st.success("ğŸ§  ë£¨ (GPT-4o)")
                st.markdown(chat['o_resp'])
            st.markdown("---") # êµ¬ë¶„ì„ 

        with tab2:
            st.markdown(f"### Q{idx}ì— ëŒ€í•œ ë¶„ì„")
            col1, col2 = st.columns(2)
            with col1:
                st.info("ğŸ’ ë‹¤ì˜¨ì˜ ë¹„í‰")
                st.markdown(chat['g_an'])
            with col2:
                st.success("ğŸ§  ë£¨ì˜ í‰ê°€")
                st.markdown(chat['o_an'])
            st.markdown("---")

        with tab3:
            st.markdown(f"### ğŸ† Q{idx} ìµœì¢… íŒê²°")
            st.markdown(chat['final_con'])
            st.markdown("---")

else:
    # ëŒ€í™” ê¸°ë¡ì´ ì—†ì„ ë•Œ ì´ˆê¸° í™”ë©´
    st.info("ğŸ‘‹ Pro ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì‹¬ì¸µ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
