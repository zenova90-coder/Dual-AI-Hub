import streamlit as st
import google.generativeai as genai
from openai import OpenAI
from datetime import datetime
import json
import os
import concurrent.futures # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Dual-AI Hub (Speed)", layout="wide")
st.title("Dual-AI Insight Hub")

# --- 2. API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
except KeyError:
    st.error("ğŸš¨ API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=gemini_api_key)
gpt_client = OpenAI(api_key=gpt_api_key)

# --- 3. ëª¨ë¸ ì„¤ì • (ì†ë„ ìµœì í™”) ---
def get_best_available_model():
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        priority_keywords = ['1.5-flash', '1.5-pro', 'gemini-pro']
        for keyword in priority_keywords:
            for m in models:
                if keyword in m: return m
        return models[0] if models else "models/gemini-pro"
    except: return "models/gemini-pro"

GEMINI_MODEL = get_best_available_model()
GPT_MODEL = "gpt-4o-mini" 

# --- 4. ë°ì´í„° ê´€ë¦¬ ---
DB_FILE = "chat_db.json"

def load_data():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
    return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]

def save_data(sessions):
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(sessions, f, ensure_ascii=False, indent=4)

# --- 5. ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ---
if "sessions" not in st.session_state:
    st.session_state.sessions = load_data()
    st.session_state.active_index = 0

if "system_role" not in st.session_state:
    st.session_state.system_role = "ë„ˆëŠ” ê° ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ë‹¤. ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ë¼."

if "active_index" not in st.session_state:
    st.session_state.active_index = 0

def get_active_session():
    if st.session_state.active_index >= len(st.session_state.sessions):
        st.session_state.active_index = 0
    return st.session_state.sessions[st.session_state.active_index]

# --- 6. [í•µì‹¬] ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---
def call_gemini(prompt):
    model = genai.GenerativeModel(GEMINI_MODEL)
    return model.generate_content(prompt).text

def call_gpt(messages):
    response = gpt_client.chat.completions.create(
        model=GPT_MODEL,
        messages=messages
    )
    return response.choices[0].message.content

# --- 7. ì‚¬ì´ë“œë°” (UI ìˆ˜ì • ì ìš©ë¨) ---
with st.sidebar:
    st.header("ğŸ­ AI ì—­í• ") 
    
    # [ìˆ˜ì •] ë¼ë²¨ì„ ìˆ¨ê¸°ê³  ì˜ˆì‹œ ë¬¸êµ¬(placeholder) ì¶”ê°€
    input_role = st.text_area(
        "role_input_hidden", # ë‚´ë¶€ ì‹ë³„ìš© ë¼ë²¨
        value=st.session_state.system_role,
        height=100,
        label_visibility="collapsed", # ìƒë‹¨ ë¼ë²¨ ë¬¸êµ¬ ìˆ¨ê¹€
        placeholder="AIê°€ ìˆ˜í–‰í•  ì›í•˜ëŠ” ì—­í• ì„ ì…ë ¥í•˜ì„¸ìš”" # ë¹ˆ ì¹¸ì¼ ë•Œ í‘œì‹œë  ì˜ˆì‹œ ë¬¸êµ¬
    )
    
    if st.button("ğŸ’¾ ì—­í•  ì ìš©í•˜ê¸°", use_container_width=True):
        st.session_state.system_role = input_role
        st.success("âœ… ì—­í•  ë¶€ì—¬ ì™„ë£Œ!")

    st.divider()
    st.header("ğŸ—‚ï¸ ëŒ€í™” ê¸°ë¡")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• ìƒˆ ëŒ€í™”", use_container_width=True):
            new_session = {"title": "ìƒˆ ëŒ€í™”", "history": []}
            st.session_state.sessions.insert(0, new_session)
            st.session_state.active_index = 0
            save_data(st.session_state.sessions)
            st.rerun()
    with col2:
        if st.button("ğŸ—‘ï¸ ì „ì²´ ì‚­ì œ", use_container_width=True):
            if os.path.exists(DB_FILE): os.remove(DB_FILE)
            st.session_state.sessions = [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
            st.session_state.active_index = 0
            st.rerun()

    st.divider()
    for i, session in enumerate(st.session_state.sessions):
        label = session["title"]
        if len(label) > 12: label = label[:12] + "..."
        if i == st.session_state.active_index:
            st.button(f"ğŸ“‚ {label}", key=f"s_{i}", use_container_width=True, disabled=True)
        else:
            if st.button(f"ğŸ“„ {label}", key=f"s_{i}", use_container_width=True):
                st.session_state.active_index = i
                st.rerun()

# --- 8. ë©”ì¸ ë¡œì§ (ê¸°ì–µë ¥ ê¸°ëŠ¥ + ë³‘ë ¬ ì²˜ë¦¬) ---
active_session = get_active_session()
chat_history = active_session["history"]
current_role = st.session_state.system_role

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    if len(chat_history) == 0:
        active_session["title"] = user_input
        save_data(st.session_state.sessions)

    with st.status("âš¡ 3ë‹¨ê³„ ì‘ì—… ì§„í–‰ ì¤‘...", expanded=True) as status:
        turn_data = {"q": user_input, "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M")}
        
        # [ê¸°ì–µë ¥ ê°•í™”] ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        messages_for_gpt = [{"role": "system", "content": current_role}]
        prompt_for_gemini = f"System Instruction: {current_role}\n\n"

        # ê³¼ê±° ê¸°ë¡ ì£¼ì…
        for chat in chat_history:
            messages_for_gpt.append({"role": "user", "content": chat['q']})
            prompt_for_gemini += f"User: {chat['q']}\n"
            
            messages_for_gpt.append({"role": "assistant", "content": chat['final_con']})
            prompt_for_gemini += f"Assistant: {chat['final_con']}\n"

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages_for_gpt.append({"role": "user", "content": user_input})
        prompt_for_gemini += f"User: {user_input}\n"

        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ThreadPoolExecutor() as executor:
            
            # --- STEP 1: ë‹µë³€ ìƒì„± ---
            st.write("1ï¸âƒ£ ë‹µë³€ ìƒì„± ì¤‘...") 
            
            future_g_resp = executor.submit(call_gemini, prompt_for_gemini)
            future_o_resp = executor.submit(call_gpt, messages_for_gpt)
            
            turn_data["g_resp"] = future_g_resp.result()
            turn_data["o_resp"] = future_o_resp.result()

            # --- STEP 2: êµì°¨ ë¶„ì„ ---
            st.write("2ï¸âƒ£ ììœ  í† ë¡  ë° ë¹„í‰ ì¤‘...")
            
            g_an_prompt = f"""
            [ë‹¹ì‹ ì˜ ì—­í• ]: {current_role}
            ìœ„ ì—­í• ë¡œì„œ ìƒëŒ€ë°© AI(Chat GPT)ì˜ ë‹µë³€ì„ ê²€í† í•˜ë¼.
            
            [í˜„ì¬ ì§ˆë¬¸]: {user_input}
            [Chat GPT ë‹µë³€]: {turn_data['o_resp']}
            
            [ì§€ì‹œì‚¬í•­]:
            1. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í–ˆì„ ë•Œ ëª¨ìˆœë˜ëŠ” ì ì´ ìˆë‹¤ë©´ ì§€ì í•˜ë¼.
            2. ë‹µë³€ì„ ì½ê³  ì „ë¬¸ê°€ë¡œì„œ ëŠë¼ëŠ” ê°€ì¥ ë‚ ì¹´ë¡œìš´ í†µì°°ì´ë‚˜, ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ì— ì§‘ì¤‘í•˜ë¼.
            3. ëŒ€í™”í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë¹„í‰í•˜ë¼.
            """
            future_g_an = executor.submit(call_gemini, g_an_prompt)
            
            o_an_messages = [
                {"role": "system", "content": current_role},
                {"role": "user", "content": f"""
                ë‹¤ìŒ Geminiì˜ ë‹µë³€ì„ í‰ê°€í•˜ë¼.
                
                [í˜„ì¬ ì§ˆë¬¸]: {user_input}
                [Gemini ë‹µë³€]: {turn_data['g_resp']}
                
                [ì§€ì‹œì‚¬í•­]:
                1. ì´ ë‹µë³€ì´ '{user_input}'ì´ë¼ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìˆì–´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ì¸ì§€ ë¹„í‰í•˜ë¼.
                2. ì´ì „ ëŒ€í™” íë¦„ìƒ ì–´ìƒ‰í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì§€ì í•˜ë¼.
                3. ë™ë£Œ ì „ë¬¸ê°€ì—ê²Œ í”¼ë“œë°±ì„ ì£¼ë“¯ êµ¬ì²´ì ìœ¼ë¡œ ë§í•˜ë¼.
                """}
            ]
            future_o_an = executor.submit(call_gpt, o_an_messages)
            
            turn_data["g_an"] = future_g_an.result()
            turn_data["o_an"] = future_o_an.result()

            # --- STEP 3: ìµœì¢… ê²°ë¡  ---
            st.write("3ï¸âƒ£ ìµœì¢… ê²°ë¡  ë„ì¶œ ì¤‘...")
            
            final_prompt = f"""
            ë‹¹ì‹ ì€ {current_role} ì—­í• ì„ ë§¡ì€ ìµœì¢… ì˜ì‚¬ê²°ì •ê¶Œìì…ë‹ˆë‹¤.
            ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ë©°, ì´ë²ˆ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì ì˜ ì†”ë£¨ì…˜ì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
            
            [í˜„ì¬ ì§ˆë¬¸]: {user_input}
            [Gemini ì˜ê²¬]: {turn_data['g_resp']}
            [GPT ì˜ê²¬]: {turn_data['o_resp']}
            [Gemini ë¹„í‰]: {turn_data['g_an']}
            [GPT ë¹„í‰]: {turn_data['o_an']}
            
            ìœ„ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ê²°ë¡ ì„ ë‚´ë¦¬ì‹­ì‹œì˜¤.
            """
            
            turn_data["final_con"] = call_gpt([{"role": "user", "content": final_prompt}])

            active_session["history"].append(turn_data)
            save_data(st.session_state.sessions)
            
            status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
            st.rerun()

# --- 9. í™”ë©´ ì¶œë ¥ ---
if chat_history:
    st.caption(f"ğŸ•’ í˜„ì¬ ëŒ€í™”: {len(chat_history)}ê°œì˜ ë¶„ì„ ê¸°ë¡")
    total_count = len(chat_history)
    
    for i, chat in enumerate(reversed(chat_history)):
        idx = total_count - i
        st.markdown(f"### Q{idx}. {chat['q']}")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì˜ê²¬ ì œì‹œ", "âš”ï¸ êµì°¨ ê²€ì¦", "ğŸ† ìµœì¢… ê²°ë¡ "])
        
        with tab1:
            c1, c2 = st.columns(2)
            with c1: 
                st.info("ğŸ’ ë‹¤ì˜¨ (Gemini)")
                st.write(chat['g_resp'])
            with c2: 
                st.success("ğŸ§  ë£¨ (Chat GPT)")
                st.write(chat['o_resp'])
        with tab2:
            c1, c2 = st.columns(2)
            with c1: 
                st.info("ë¹„í‰")
                st.write(chat['g_an'])
            with c2: 
                st.success("í‰ê°€")
                st.write(chat['o_an'])
        with tab3:
            st.markdown(chat['final_con'])
        st.divider()
