import streamlit as st
import google.generativeai as genai
from openai import OpenAI
from datetime import datetime
import json
import os
import concurrent.futures
import PyPDF2
from io import StringIO

# --- 1. í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰) ---
st.set_page_config(page_title="Dual-AI Hub (Private)", layout="wide")

# ==========================================
# ğŸ”’ [ë³´ì•ˆ] ë¹„ë°€ë²ˆí˜¸ ì ê¸ˆ ì¥ì¹˜ (ë¬¸ì§€ê¸°)
# ==========================================
def check_password():
    """ë¹„ë°€ë²ˆí˜¸ê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    if st.session_state.get("password_correct", False):
        return True

    st.header("ğŸ”’ ì ‘ì† ê¶Œí•œ í™•ì¸")
    st.write("ê´€ë¦¬ìê°€ ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    password_input = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
    
    if st.button("ë¡œê·¸ì¸"):
        try:
            # secretsì— ì„¤ì •ëœ ë¹„ë°€ë²ˆí˜¸ì™€ ë¹„êµ
            if password_input == st.secrets["APP_PASSWORD"]:
                st.session_state["password_correct"] = True
                st.rerun()
            else:
                st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        except KeyError:
            st.error("ğŸš¨ ì„œë²„ì— ë¹„ë°€ë²ˆí˜¸(APP_PASSWORD) ì„¤ì •ì´ ì•ˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    return False

# ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦¬ë©´ ì—¬ê¸°ì„œ ì½”ë“œ ì‹¤í–‰ì„ ë©ˆì¶¤ (ì•„ë˜ ë‚´ìš© ì ˆëŒ€ ì•ˆ ë³´ì—¬ì¤Œ)
if not check_password():
    st.stop()

# ==========================================
# ğŸ”“ ë¡œê·¸ì¸ ì„±ê³µ í›„ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ì½”ë“œ
# ==========================================

st.title("âš¡ Dual-AI Insight Hub (Private)")

# --- 2. API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
except KeyError:
    st.error("ğŸš¨ API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. (.streamlit/secrets.toml í™•ì¸)")
    st.stop()

genai.configure(api_key=gemini_api_key)
gpt_client = OpenAI(api_key=gpt_api_key)

# --- 3. ëª¨ë¸ ì„¤ì • (ìë™ íƒìƒ‰) ---
def get_best_available_model():
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        priority_keywords = ['1.5-flash', '1.5-pro', 'gemini-pro']
        for keyword in priority_keywords:
            for m in models:
                if keyword in m: return m
        return models[0] if models else "models/gemini-pro"
    except: return "models/gemini-pro"

GEMINI_MODEL = get_best_available_model()
GPT_MODEL = "gpt-4o-mini" # ì†ë„ì™€ ê°€ì„±ë¹„ ìµœê°•

# --- 4. ë°ì´í„° ê´€ë¦¬ (íŒŒì¼ ì €ì¥) ---
DB_FILE = "chat_db.json"

def load_data():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
    return [{"title": "ìƒˆ ëŒ€í™”", "history": []}]

def save_data(sessions):
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(sessions, f, ensure_ascii=False, indent=4)

# --- 5. íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (PDF/TXT) ---
def process_uploaded_file(uploaded_file):
    try:
        text = ""
        if uploaded_file.type == "application/pdf":
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                extracted = page.extract_text()
                if extracted: text += extracted + "\n"
        else: # í…ìŠ¤íŠ¸, ì½”ë“œ, ë§ˆí¬ë‹¤ìš´ ë“±
            stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
            text = stringio.read()
            
        if not text.strip():
            return None, "âš ï¸ íŒŒì¼ ë‚´ìš©ì€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ì´ë¯¸ì§€ ìŠ¤ìº”ë³¸ PDFì¼ ìˆ˜ ìˆìŒ)"
        return text, f"âœ… ë¡œë“œ ì„±ê³µ! ({len(text)}ì)"
    except Exception as e:
        return None, f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"

# --- 6. ì„¸ì…˜ ë° ìºì‹œ ì´ˆê¸°í™” ---
if "sessions" not in st.session_state:
    st.session_state.sessions = load_data()
    st.session_state.active_index = 0
if "system_role" not in st.session_state:
    st.session_state.system_role = "ë„ˆëŠ” ê° ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ë‹¤."
if "active_index" not in st.session_state:
    st.session_state.active_index = 0
if "file_cache" not in st.session_state:
    st.session_state.file_cache = {"name": None, "content": None}

def get_active_session():
    if st.session_state.active_index >= len(st.session_state.sessions):
        st.session_state.active_index = 0
    return st.session_state.sessions[st.session_state.active_index]

# --- 7. API í˜¸ì¶œ í•¨ìˆ˜ ---
def call_gemini(prompt):
    model = genai.GenerativeModel(GEMINI_MODEL)
    return model.generate_content(prompt).text

def call_gpt(messages):
    response = gpt_client.chat.completions.create(model=GPT_MODEL, messages=messages)
    return response.choices[0].message.content

# --- 8. ì‚¬ì´ë“œë°” (ì»¨íŠ¸ë¡¤ íŒ¨ë„) ---
with st.sidebar:
    st.success("ğŸ” ë¡œê·¸ì¸ ì™„ë£Œ") # ë¡œê·¸ì¸ ì„±ê³µ í‘œì‹œ
    st.header("ğŸ® ì œì–´ ì„¼í„°")
    
    # [1] ìë£Œ ì—…ë¡œë“œ
    st.subheader("ğŸ“‚ ìë£Œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=["pdf", "txt", "csv", "py", "md"])
    
    if uploaded_file:
        if st.session_state.file_cache["name"] != uploaded_file.name:
            with st.spinner("íŒŒì¼ ë¶„ì„ ì¤‘..."):
                content, msg = process_uploaded_file(uploaded_file)
                if content:
                    st.session_state.file_cache = {"name": uploaded_file.name, "content": content}
                    st.success(msg)
                else:
                    st.error(msg)
        else:
            st.success(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìœ ì§€ ì¤‘: {uploaded_file.name}")
    else:
        st.session_state.file_cache = {"name": None, "content": None}

    st.divider()

    # [2] í˜ë¥´ì†Œë‚˜
    with st.expander("ğŸ­ AI ì—­í•  ì„¤ì •"):
        input_role = st.text_area("ì—­í• ", value=st.session_state.system_role)
        if st.button("ğŸ’¾ ì—­í•  ì ìš©"):
            st.session_state.system_role = input_role
            st.success("ì ìš©ë¨")

    st.divider()

    # [3] ëŒ€í™”ë°© ê´€ë¦¬
    st.subheader("ğŸ—‚ï¸ ëŒ€í™”ë°©")
    active_session = get_active_session()
    
    new_title = st.text_input("ğŸ·ï¸ ë°© ì´ë¦„ ìˆ˜ì •", value=active_session["title"], key=f"title_{st.session_state.active_index}")
    if new_title != active_session["title"]:
        active_session["title"] = new_title
        save_data(st.session_state.sessions)
        st.rerun()

    c1, c2 = st.columns(2)
    with c1: 
        if st.button("â• ìƒˆ ëŒ€í™”"):
            st.session_state.sessions.insert(0, {"title": "ìƒˆ ëŒ€í™”", "history": []})
            st.session_state.active_index = 0
            save_data(st.session_state.sessions)
            st.rerun()
    with c2:
        if st.button("ğŸ—‘ï¸ ì‚­ì œ"):
            st.session_state.sessions = [{"title": "ìƒˆ ëŒ€í™”", "history": []}]
            st.session_state.active_index = 0
            if os.path.exists(DB_FILE): os.remove(DB_FILE)
            st.rerun()

    st.markdown("---")
    for i, session in enumerate(st.session_state.sessions):
        label = session["title"][:15] + "..." if len(session["title"]) > 15 else session["title"]
        if i == st.session_state.active_index:
            st.button(f"ğŸ“‚ {label}", key=f"s{i}", disabled=True)
        else:
            if st.button(f"ğŸ“„ {label}", key=f"s{i}"):
                st.session_state.active_index = i
                st.rerun()

# --- 9. ë©”ì¸ ë¡œì§ (ë³‘ë ¬ ì²˜ë¦¬ + íŒŒì¼ ë¶„ì„) ---
active_session = get_active_session()
chat_history = active_session["history"]
current_role = st.session_state.system_role
current_file_content = st.session_state.file_cache["content"]

# ë¶„ì„ íŠ¸ë¦¬ê±°
trigger_analysis = False
auto_prompt = ""

if current_file_content:
    st.info(f"ğŸ“ **{st.session_state.file_cache['name']}** ë‚´ìš©ì„ ì°¸ì¡°í•©ë‹ˆë‹¤.")
    if st.button("ğŸ“‘ íŒŒì¼ ìš”ì•½ ë° ë¶„ì„ ì‹¤í–‰", use_container_width=True):
        trigger_analysis = True
        auto_prompt = "ì´ íŒŒì¼ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë¶„ì„í•´ì¤˜."

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input or trigger_analysis:
    final_question = user_input if user_input else auto_prompt

    # ì²« ëŒ€í™”ë©´ ì œëª© ìë™ ë³€ê²½
    if len(chat_history) == 0 and active_session["title"] == "ìƒˆ ëŒ€í™”":
        active_session["title"] = final_question[:20]
        save_data(st.session_state.sessions)
        st.rerun()

    with st.status("âš¡ ë³´ì•ˆ ì ‘ì† ì¤‘... AIê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.", expanded=True) as status:
        try:
            turn_data = {"q": final_question, "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M")}
            
            # íŒŒì¼ ë‚´ìš©ì´ ìˆë‹¤ë©´ í”„ë¡¬í”„íŠ¸ì— ê²°í•© (ìµœëŒ€ 3ë§Œì)
            context_input = final_question
            if current_file_content:
                safe_content = current_file_content[:30000]
                context_input = f"""
                [ì°¸ê³  ìë£Œ (íŒŒì¼)]:
                {safe_content}
                ...(ìƒëµë¨)...
                
                [ì‚¬ìš©ì ìš”ì²­]: {final_question}
                """

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
            with concurrent.futures.ThreadPoolExecutor() as executor:
                # 1. ë‹µë³€
                st.write(f"1ï¸âƒ£ ë‹µë³€ ìƒì„± ì¤‘... (Role: {current_role[:10]})")
                f_g = executor.submit(call_gemini, f"System: {current_role}\n\n{context_input}")
                f_o = executor.submit(call_gpt, [{"role": "system", "content": current_role}, {"role": "user", "content": context_input}])
                
                turn_data["g_resp"] = f_g.result()
                turn_data["o_resp"] = f_o.result()

                # 2. ë¹„í‰
                st.write("2ï¸âƒ£ êµì°¨ ê²€ì¦ ì¤‘...")
                f_g_an = executor.submit(call_gemini, f"Role: {current_role}\nEvaluate GPT's answer. Don't use Pros/Cons list.\n\nGPT Answer: {turn_data['o_resp']}")
                f_o_an = executor.submit(call_gpt, [{"role": "system", "content": current_role}, {"role": "user", "content": f"Evaluate Gemini's answer. Don't use Pros/Cons list.\n\nGemini Answer: {turn_data['g_resp']}"}])
                
                turn_data["g_an"] = f_g_an.result()
                turn_data["o_an"] = f_o_an.result()

                # 3. ê²°ë¡ 
                st.write("3ï¸âƒ£ ìµœì¢… ê²°ë¡  ë„ì¶œ...")
                final_p = f"""
                Role: {current_role}
                Task: Synthesize final conclusion. Fix errors found in review.
                
                Q: {final_question}
                Gemini: {turn_data['g_resp']}
                GPT: {turn_data['o_resp']}
                Review(G): {turn_data['g_an']}
                Review(O): {turn_data['o_an']}
                """
                turn_data["final_con"] = call_gpt([{"role": "user", "content": final_p}])

                active_session["history"].append(turn_data)
                save_data(st.session_state.sessions)
                
                status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
                st.rerun()

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- 10. ê²°ê³¼ ì¶œë ¥ ---
if chat_history:
    st.caption(f"ğŸ•’ ê¸°ë¡: {len(chat_history)}ê±´ | í˜„ì¬ ë°©: {active_session['title']}")
    for i, chat in enumerate(reversed(chat_history)):
        idx = len(chat_history) - i
        st.markdown(f"### Q{idx}. {chat['q']}")
        t1, t2, t3 = st.tabs(["ğŸ’¬ ë‹µë³€", "âš”ï¸ ë¹„í‰", "ğŸ† ê²°ë¡ "])
        with t1:
            c1, c2 = st.columns(2)
            with c1: st.info("ğŸ’ ë‹¤ì˜¨"); st.write(chat['g_resp'])
            with c2: st.success("ğŸ§  ë£¨"); st.write(chat['o_resp'])
        with t2:
            c1, c2 = st.columns(2)
            with c1: st.info("ë¹„í‰"); st.write(chat['g_an'])
            with c2: st.success("í‰ê°€"); st.write(chat['o_an'])
        with t3: st.markdown(chat['final_con'])
        st.divider()
