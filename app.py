import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import json
import os
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Dual-AI Hub", layout="wide")
st.title("ğŸ¤– Dual-AI Insight Hub")

# --- 1. API í‚¤ ì„¤ì • ë° ì—°ê²° ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    gpt_api_key = st.secrets["GPT_API_KEY"]
    
    genai.configure(api_key=gemini_api_key)
    gpt_client = OpenAI(api_key=gpt_api_key)
except Exception as e:
    st.error(f"ğŸš¨ API í‚¤ ì„¤ì • ì˜¤ë¥˜: {e}")
    st.stop()

# --- 2. ë‹¥í„° ë‹¤ì˜¨: ì‘ë™í•˜ëŠ” ëª¨ë¸ ìë™ ì°¾ê¸° ---
def get_valid_gemini_model():
    try:
        # ì„œë²„ì—ê²Œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë‹¬ë¼ê³  ìš”ì²­
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        # ìš°ì„ ìˆœìœ„: 1.5-flash (ë¹ ë¦„) -> 1.5-pro (ë˜‘ë˜‘í•¨) -> pro (ì•ˆì •ì )
        preferred = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-pro']
        
        for p in preferred:
            if p in models:
                return p # ì°¾ì•˜ë‹¤!
        
        return models[0] if models else None # ì—†ìœ¼ë©´ ì•„ë¬´ê±°ë‚˜ ì²«ë²ˆì§¸
    except:
        return 'gemini-pro' # ëª©ë¡ ì¡°íšŒê°€ ì•ˆë˜ë©´ ê¸°ë³¸ê°’ ê°•ì œ ì‚¬ìš©

# ì•ˆì „í•œ ëª¨ë¸ ì´ë¦„ í™•ë³´
valid_model_name = get_valid_gemini_model()

# --- 3. íˆìŠ¤í† ë¦¬(ê¸°ë¡) ê´€ë¦¬ ê¸°ëŠ¥ ---
HISTORY_FILE = "chat_history.json"

def save_session_history(session_data):
    if not session_data: return
    
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = json.load(f)
    else:
        history = []
    
    # ê¸°ë¡ ì–‘ì‹: ì‹œê°„ + ì²« ì§ˆë¬¸ ì œëª© + ì „ì²´ ëŒ€í™” ë‚´ìš©
    record = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "title": session_data[0]['q'][:15] + "...",
        "dialogue": session_data
    }
    
    history.insert(0, record)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def load_history_list():
    if not os.path.exists(HISTORY_FILE): return []
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return []

# --- 4. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "current_chat_log" not in st.session_state: 
    st.session_state.current_chat_log = []

# --- 5. ì‚¬ì´ë“œë°” (ê¸°ë¡ ë³´ê´€ì†Œ) ---
with st.sidebar:
    st.header("ğŸ—‚ï¸ ëŒ€í™” ê¸°ë¡")
    
    # [ìƒˆ ëŒ€í™” ì‹œì‘] ë²„íŠ¼
    if st.button("â• ìƒˆ ëŒ€í™” ì‹œì‘ (í™”ë©´ ì´ˆê¸°í™”)", use_container_width=True):
        if st.session_state.current_chat_log:
            save_session_history(st.session_state.current_chat_log)
            st.toast("ì´ì „ ëŒ€í™”ê°€ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        st.session_state.current_chat_log = [] # í™”ë©´ ë¹„ìš°ê¸°
        st.rerun()

    st.divider()
    
    # ì €ì¥ëœ ê¸°ë¡ ëª©ë¡
    history_list = load_history_list()
    if not history_list:
        st.caption("ì•„ì§ ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for idx, item in enumerate(history_list):
            # ë²„íŠ¼ ëˆ„ë¥´ë©´ ê³¼ê±° ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
            if st.button(f"ğŸ“„ {item['timestamp']} | {item['title']}", key=f"hist_{idx}"):
                st.session_state.current_chat_log = item['dialogue']
                st.rerun()
                
    st.divider()
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì‚­ì œ"):
        if os.path.exists(HISTORY_FILE):
            os.remove(HISTORY_FILE)
            st.session_state.current_chat_log = []
            st.rerun()

# --- 6. ë©”ì¸ í™”ë©´: 3ë‹¨ê³„ íƒ­ êµ¬ì„± ---
# íƒ­ì„ ë¯¸ë¦¬ ì •ì˜
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ 1. ë‹µë³€ (Opinions)", "âš”ï¸ 2. êµì°¨ ë¶„ì„ (Cross-Analysis)", "ğŸ† 3. ìµœì¢… ê²°ë¡  (Conclusion)"])

# ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë‚´ìš© í‘œì‹œ (ëˆ„ì  ë°©ì‹)
if st.session_state.current_chat_log:
    
    # [Tab 1] ì§ˆë¬¸ & ë‹µë³€
    with tab1:
        for i, log in enumerate(st.session_state.current_chat_log):
            st.markdown(f"**Q{i+1}. {log['q']}**") 
            c1, c2 = st.columns(2)
            c1.info(f"ğŸ’ ë‹¤ì˜¨")
            c1.write(log['g_resp'])
            c2.success(f"ğŸ§  ë£¨")
            c2.write(log['o_resp'])
            st.divider()

    # [Tab 2] êµì°¨ ë¶„ì„
    with tab2:
        for i, log in enumerate(st.session_state.current_chat_log):
            st.markdown(f"**Q{i+1} ë¶„ì„**")
            c1, c2 = st.columns(2)
            c1.info("ğŸ’ ë‹¤ì˜¨ì˜ ë¹„í‰")
            c1.write(log['g_an'])
            c2.success("ğŸ§  ë£¨ì˜ í‰ê°€")
            c2.write(log['o_an'])
            st.divider()

    # [Tab 3] ìµœì¢… ê²°ë¡ 
    with tab3:
        for i, log in enumerate(st.session_state.current_chat_log):
            st.markdown(f"**Q{i+1} ê²°ë¡ **")
            st.markdown(log['final_con'])
            st.divider()

# --- 7. ì…ë ¥ì°½ ë° ì‹¤í–‰ ë¡œì§ ---
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ìë™ìœ¼ë¡œ 3ë‹¨ê³„ ë¶„ì„ì´ ì§„í–‰ë©ë‹ˆë‹¤)")

if user_input:
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    with st.status("ğŸš€ ë‹¤ì˜¨ê³¼ ë£¨ê°€ ì—´ì‹¬íˆ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
        new_turn = {"q": user_input, "timestamp": datetime.now().strftime("%H:%M")}
        
        # STEP 1: ë‹µë³€ ìƒì„±
        st.write("1ï¸âƒ£ ë‹µë³€ ì‘ì„± ì¤‘...")
        try:
            # ì•ˆì „í•˜ê²Œ ì°¾ì€ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            model = genai.GenerativeModel(valid_model_name.replace('models/', ''))
            new_turn["g_resp"] = model.generate_content(user_input).text
        except Exception as e:
            new_turn["g_resp"] = f"Gemini ì—°ê²° ì˜¤ë¥˜: {e}"
        
        try:
            res = gpt_client.chat.completions.create(model="gpt-4o", messages=[{"role":"user","content":user_input}])
            new_turn["o_resp"] = res.choices[0].message.content
        except Exception as e:
            new_turn["o_resp"] = f"GPT ì—°ê²° ì˜¤ë¥˜: {e}"

        # STEP 2: êµì°¨ ë¶„ì„
        st.write("2ï¸âƒ£ ìƒí˜¸ ë¹„íŒ ì¤‘...")
        try:
            new_turn["g_an"] = model.generate_content(f"ë‹¤ìŒì€ 'ë£¨(GPT)'ì˜ ë‹µë³€ì…ë‹ˆë‹¤. ë…¼ë¦¬ì  í—ˆì ì„ ë¹„íŒí•´ì£¼ì„¸ìš”:\n{new_turn['o_resp']}").text
        except: new_turn["g_an"] = "ë¶„ì„ ì‹¤íŒ¨"
        
        try:
            res = gpt_client.chat.completions.create(model="gpt-4o", messages=[{"role":"user","content":f"ë‹¤ìŒì€ 'ë‹¤ì˜¨(Gemini)'ì˜ ë‹µë³€ì…ë‹ˆë‹¤. ì°½ì˜ì„±ê³¼ ë…¼ë¦¬ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”:\n{new_turn['g_resp']}"}])
            new_turn["o_an"] = res.choices[0].message.content
        except: new_turn["o_an"] = "ë¶„ì„ ì‹¤íŒ¨"
        
        # STEP 3: ìµœì¢… ê²°ë¡ 
        st.write("3ï¸âƒ£ ìµœì¢… ê²°ë¡  ë„ì¶œ ì¤‘...")
        try:
            final_prompt = f"""
            ì§ˆë¬¸: {user_input}
            [ë‹¤ì˜¨ ë‹µë³€] {new_turn['g_resp']}
            [ë£¨ ë‹µë³€] {new_turn['o_resp']}
            [ë‹¤ì˜¨ ë¹„í‰] {new_turn['g_an']}
            [ë£¨ ë¹„í‰] {new_turn['o_an']}
            
            ìœ„ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ëª…ì¾Œí•œ ìµœì¢… ê²°ë¡ ì„ ë‚´ë ¤ë¼.
            """
            res = gpt_client.chat.completions.create(model="gpt-4o", messages=[{"role":"user","content":final_prompt}])
            new_turn["final_con"] = res.choices[0].message.content
        except: new_turn["final_con"] = "ê²°ë¡  ë„ì¶œ ì‹¤íŒ¨"

        # ê²°ê³¼ ì €ì¥ ë° í™”ë©´ ê°±ì‹ 
        st.session_state.current_chat_log.append(new_turn)
        status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
        st.rerun()
